{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `langchain`에서는 txt 파일 뿐만 아니라 csv나 pdf, json 등 다양한 포맷의 document를 load할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../dataset/llamaindex_data/openai.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".\\nAs one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\\n\\n\\n== History ==\\n\\n\\n=== 2015–2018: Non-profit beginnings ===\\nIn December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Research announced the formation of OpenAI and pledged over $1 billion to the venture. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, Musk was its largest donor while YC Research did not contribute anything at all. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI is headquartered at the Pioneer Building in Mission District, San Francisco.According to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.Microsoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.In April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours. In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI\\'s general intelligence across the world\\'s supply of games, websites, and other applications.In 2017 OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone. In comparison, DeepMind\\'s total expenses in 2017 were $442 million. In the summer of 2018, simply training OpenAI\\'s Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.\\nIn 2018, Musk resigned from his Board of Directors seat, citing \"a potential future conflict [of interest]\" with his role as CEO of Tesla due to Tesla\\'s AI development for self-driving cars. Sam Altman claims that Musk believed OpenAI had fallen behind other players like Google and Musk proposed instead to take over OpenAI himself, which the board rejected. Musk subsequently left OpenAI but claimed to remain a donor, yet made no donations after his departure.In February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like text.\\n\\n\\n=== 2019: Transition from non-profit ===\\nIn 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook , which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.OpenAI Global LLC then announced its intention to commercially license its technologies. It planned to spend the $1 billion \"within five years, and possibly much faster.\" Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated \"I disagree with the notion that a nonprofit can\\'t compete\" and pointed to successful low-budget projects by OpenAI and others. \"If bigger and better funded was always better, then IBM would still be number one.\"\\nThe nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.\\'s nonprofit charter. A majority of OpenAI, Inc.\\'s board is barred from having financial stakes in OpenAI Global LLC. In addition, minority members with a stake in OpenAI Global LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global LLC\\'s switch to for-profit status is inconsistent with OpenAI\\'s claims to be \"democratizing\" AI.\\n\\n\\n=== 2020–2023: ChatGPT, DALL-E, partnership with Microsoft ===\\nIn 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply \"the API\", would form the heart of its first commercial product.In 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex digital images from textual descriptions, utilizing a variant of the GPT-3 architecture.In December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days. According to anonymous sources cited by Reuters in December 2022, OpenAI Global LLC was projecting $200 million of revenue in 2023 and $1 billion in revenue in 2024.In January 2023, OpenAI Global LLC was in talks for funding that would value the company at $29 billion, double its 2021 value. On January 23, 2023, Microsoft announced a new US$10 billion investment in OpenAI Global LLC over multiple years, partially needed to use Microsoft\\'s cloud-computing service Azure. Rumors of this deal suggested that Microsoft may receive 75% of OpenAI\\'s profits until it secures its investment return and a 49% stake in the company. The investment is believed to be a part of Microsoft\\'s efforts to integrate OpenAI\\'s ChatGPT into the Bing search engine. Google announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google\\'s place as a go-to source for information.On February 7, 2023, Microsoft announced that it was building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.On March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest with his investments in AI companies via Greylock Partners, and his co-founding of the AI startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.On March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.On May 22, 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the governance of superintelligence. They consider that superintelligence could happen within the next 10 years, allowing a \"dramatically more prosperous future\" and that \"given the possibility of existential risk, we can\\'t just be reactive\". They propose creating an international watchdog organization similar to IAEA to oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on the other side should not be overly regulated. They also call for more technical safety research for superintelligences, and ask for more coordination, for example through governments launching a joint project which \"many current efforts become part of\".In August 2023, it was announced that OpenAI had acquired the New York-based start-up, Global Illumination, a company that deploys AI to develop digital infrastructure and creative tools.On September 21, 2023, Microsoft began rebranding all variants of its Copilot to Microsoft Copilot, including the former Bing Chat and the Microsoft 365 Copilot. This strategy was followed in December 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well as a standalone Microsoft Copilot app released for Android and one released for iOS thereafter.In October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI would let G42 deploy Open AI technology.On November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of ChatGPT for specific purposes, further expanding the possibilities of AI applications across various industries. On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for ChatGPT Plus due to high demand. Access for newer subscribers re-opened a month later on December 13.\\n\\n\\n==== Brief departure of Altman and Brockman ====\\n\\nOn November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D\\'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company\\'s presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher Szymon Sidor.On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman\\'s departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn\\'t work out. The board members agreed \"in principle\" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO. The board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be joining Microsoft to lead a new advanced AI research team, but added that they were still committed to OpenAI despite recent events. Before the partnership with Microsoft was finalized, Altman gave the board another opportunity to negotiate with him. About 738 of OpenAI\\'s 770 employees, including Murati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the board did not rehire Altman and then resign. This prompted OpenAI investors to consider legal action against the board as well. In response, OpenAI management sent an internal memo to employees stating that negotiations with Altman and the board had resumed and would take some time.\\nOn November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in their prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and Lawrence Summers, with D\\'Angelo remaining. On November 22, 2023, emerging reports suggested that Sam Altman\\'s dismissal from OpenAI may have been linked to his alleged mishandling of a significant breakthrough in the organization\\'s secretive project codenamed Q*. According to sources within OpenAI, Q* is aimed at developing AI capabilities in logical and mathematical reasoning, and reportedly involves performing math on the level of grade-school students. Concerns about Altman\\'s response to this development, specifically regarding the discovery\\'s potential safety implications, were reportedly raised with the company\\'s board shortly before Altman\\'s firing. On November 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as a non-voting member to observe the company\\'s operations.In February 2024, the U.S. Securities and Exchange Commission was reportedly investigating OpenAI over whether internal company communications made by Altman were used to mislead investors; and an investigation of Altman\\'s statements, opened by the Southern New York U.S. Attorney\\'s Office the previous November, was ongoing.\\n\\n\\n=== 2024–present: Public/non-profit efforts, Sora ===\\nOn January 16, 2024, in response to intense scrutiny from regulators around the world, OpenAI announced the formation of a new Collective Alignment team that would aim to implement ideas from the public for ensuring its models would \"align to the values of humanity.\" The move was from its public program launched in May 2023. The company explained that the program would be separate from its commercial endeavors. On January 18, 2024, OpenAI announced a partnership with Arizona State University that would give it complete access to ChatGPT Enterprise. ASU plans to incorporate the technology into various aspects of its operations, including courses, tutoring and research. It is OpenAI\\'s first partnership with an educational institution.On February 15, 2024, OpenAI announced a text-to-video model named Sora, which it plans to release to the public at an unspecified date. It is currently available for red teams for managing critical harms and risks.On February 29, 2024, OpenAI and CEO Sam Altman were sued by Elon Musk, who accused them of prioritizing profits over public good, contrary to OpenAI\\'s original mission of developing AI for humanity\\'s benefit. The lawsuit cited OpenAI\\'s policy shift after partnering with Microsoft, questioning its open-source commitment and stirring the AI ethics-vs.-profit debate. In a blog post, OpenAI stated that \"Elon understood the mission did not imply open-sourcing AGI.\" In a staff memo, they also denied being a de facto Microsoft subsidiary.In a March 11, 2024, court filing, OpenAI said it was \"doing just fine without Elon Musk\" after he left the company in 2018. They also responded to Musk\\'s lawsuit, calling the billionaire’s claims \"incoherent\", \"frivolous\", \"extraordinary\" and \"a fiction\".\\n\\n\\n== Participants ==\\nKey employees\\n\\nCEO and co-founder: Sam Altman, former president of the startup accelerator Y Combinator\\nPresident and co-founder: Greg Brockman, former CTO, 3rd employee of Stripe\\nChief Scientist and co-founder: Ilya Sutskever, a former Google expert on machine learning\\nChief Technology Officer: Mira Murati, previously at Leap Motion and Tesla, Inc.\\nChief Operating Officer: Brad Lightcap, previously at Y Combinator and JPMorgan Chase\\n\\n\\n=== Board of Directors of the OpenAI nonprofit ===\\nMicrosoft (observer)\\nBret Taylor (chairman)\\nSam Altman\\nLawrence Summers\\nAdam D\\'Angelo\\nSue Desmond-Hellmann\\nNicole Seligman\\nFidji Simo\\n\\n\\n=== Principal individual investors ===\\nReid Hoffman, LinkedIn co-founder\\nPeter Thiel, PayPal co-founder\\nJessica Livingston, a founding partner of Y Combinator\\nElon Musk, co-founder\\n\\n\\n=== Corporate investors ===\\nMicrosoft\\nKhosla Ventures\\nInfosys\\nThrive Capital\\n\\n\\n== Motives ==\\nSome scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI gains the ability to redesign itself at an ever-increasing rate, an unstoppable \"intelligence explosion\" could lead to human extinction. Co-founder Musk characterizes AI as humanity\\'s \"biggest existential threat\".Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence. OpenAI states that \"it\\'s hard to fathom how much human-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". Research on safety cannot safely be postponed: \"because of AI\\'s surprising history, it\\'s hard to predict when human-level AI might come within reach.\" OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible.\" Co-chair Sam Altman expects the decades-long project to surpass human intelligence.Vishal Sikka, former CEO of Infosys, stated that an \"openness\", where the endeavor would \"produce results generally in the greater interest of humanity\", was a fundamental requirement for his support; and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\". Cade Metz of Wired suggested that corporations such as Amazon might be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook, which own enormous supplies of proprietary data. Altman stated that Y Combinator companies would share their data with OpenAI.\\n\\n\\n== Strategy ==\\nIn the early years before his 2018 departure, Musk posed the question: \"What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" He acknowledged that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; but nonetheless, that the best defense was \"to empower as many people as possible to have AI. If everyone has AI powers, then there\\'s not any one person or a small set of individuals who can have AI superpower.\"Musk and Altman\\'s counterintuitive strategy—that of trying to reduce of harm from AI by giving everyone access to it—is controversial among those concerned with existential risk from AI. Philosopher Nick Bostrom said, \"If you have a button that could do bad things to the world, you don\\'t want to give it to everyone.\" During a 2016 conversation about technological singularity, Altman said, \"We don\\'t plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated, \"Our goal right now... is to do the best thing there is to do. It\\'s a little vague.\"Conversely, OpenAI\\'s initial decision to withhold GPT-2 around 2019, due to a wish to \"err on the side of caution\" in the presence of potential misuse, was criticized by advocates of openness. Delip Rao, an expert in text generation, stated, \"I don\\'t think [OpenAI] spent enough time proving [GPT-2] was actually dangerous.\" Other critics argued that open publication was necessary to replicate the research and to create countermeasures.More recently, in 2022, OpenAI published its approach to the alignment problem, anticipating that aligning AGI to human values would likely be harder than aligning current AI systems: \"Unaligned AGI could pose substantial risks to humanity[,] and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together\". They stated that they intended to explore how to better use human feedback to train AI systems, and how to safely use AI to incrementally automate alignment research. Some observers believe the company\\'s November 2023 reorganization—including Altman\\'s return as CEO, and the changes to its board of directors—indicated a probable shift towards a business focus and reduced influence of \"cautious people\" at OpenAI.\\n\\n\\n== Products and applications ==\\n\\n\\n=== Reinforcement learning ===\\nAt its beginning, OpenAI\\'s research included many projects focused on reinforcement learning (RL). OpenAI has been viewed as an important competitor to DeepMind.\\n\\n\\n==== Gym ====\\nAnnounced in 2016, Gym aimed to provide an easily implemented general-intelligence benchmark in a wide variety of environments—akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research. It sought to standardize how environments were defined in AI research publications, so that published research became more easily reproducible, and to provide users with a simple interface. As of June 2017, Gym could be used only with Python. As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its GitHub page.\\n\\n\\n==== Gym Retro ====\\nReleased in 2018, Gym Retro is a platform for reinforcement learning (RL) research on video games, using RL algorithms and study generalization. Prior RL research focused mainly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.\\n\\n\\n==== RoboSumo ====\\nReleased in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack knowledge of how to even walk, but are given the goals of learning to move and to push the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions. When an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way. OpenAI\\'s Igor Mordatch argued that competition between agents could create an intelligence \"arms race\" that could increase an agent\\'s ability to function even outside the context of the competition.\\n\\n\\n==== OpenAI Five ====\\n\\nOpenAI Five is a team of five OpenAI-curated bots used in the competitive five-on-five video game Dota 2, that learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon. The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.By June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games. In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco. The bots\\' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.OpenAI Five\\'s mechanisms in Dota 2\\'s bot player shows the challenges of AI systems in multiplayer online battle arena (MOBA) games and how OpenAI Five has demonstrated the use of deep reinforcement learning (DRL) agents to achieve superhuman competence in Dota 2 matches.\\n\\n\\n==== Dactyl ====\\nDeveloped in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects. It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.In 2019, OpenAI demonstrated that Dactyl could solve a Rubik\\'s Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik\\'s Cube introduce complex physics that is harder to model. OpenAI did this by improving the robustness of Dactyl to perturbations by using Automatic Domain Randomization (ADR), a simulation approach of generating progressively more difficult environments. ADR differs from manual domain randomization by not needing a human to specify randomization ranges.\\n\\n\\n=== API ===\\nIn June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task\".\\n\\n\\n=== Text generation ===\\nThe company has popularized generative pretrained transformers (GPT).\\n\\n\\n==== OpenAI\\'s original GPT model (\"GPT-1\") ====\\n\\nThe original paper on generative pre-training of a transformer-based language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI\\'s website on June 11, 2018. It showed how a generative model of language could acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.\\n\\n\\n==== GPT-2 ====\\n\\nGenerative Pre-trained Transformer 2 (\"GPT-2\") is an unsupervised transformer language model and the successor to OpenAI\\'s original GPT model (\"GPT-1\"). GPT-2 was announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released due to concern about potential misuse, including applications for writing fake news. Some experts expressed skepticism that GPT-2 posed a significant threat.\\nIn response to GPT-2, the Allen Institute for Artificial Intelligence responded with a tool to detect \"neural fake news\". Other researchers, such as Jeremy Howard, warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\". In November 2019, OpenAI released the complete version of the GPT-2 language model. Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.GPT-2\\'s authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples).\\nThe corpus it was trained on, called WebText, contains slightly 40 gigabytes of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This permits representing any string of characters by encoding both individual characters and multiple-character tokens.\\n\\n\\n==== GPT-3 ====\\n\\nFirst described in May 2020, Generative Pre-trained Transformer 3 (GPT-3) is an unsupervised transformer language model and the successor to GPT-2. OpenAI stated that the full version of GPT-3 contained 175 billion parameters, two orders of magnitude larger than the 1.5 billion in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).OpenAI stated that GPT-3 succeeded at certain \"meta-learning\" tasks and could generalize the purpose of a single input-output pair. The GPT-3 release paper gave examples of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.GPT-3 dramatically improved benchmark results  over GPT-2. OpenAI cautioned that such scaling-up of language models could be approaching or encountering the fundamental capability limitations of predictive language models. Pre-training GPT-3 required several thousand petaflop/s-days of compute, compared to tens of petaflop/s-days for the full GPT-2 model. Like its predecessor, the GPT-3 trained model was not immediately released to the public for concerns of possible abuse, although OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.\\n\\n\\n==== Codex ====\\n\\nAnnounced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories, and is the AI powering the code autocompletion tool GitHub Copilot. In August 2021, an API was released in private beta. According to OpenAI, the model can create working code in over a dozen programming languages, most effectively in Python.Several issues with glitches, design flaws and security vulnerabilities were cited.GitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.OpenAI announced that they would discontinue support for Codex API on March 23, 2023.\\n\\n\\n==== GPT-4 ====\\n\\nOn March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs. They announced that the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5 scored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n\\n=== Image classification ===\\n\\n\\n==== CLIP ====\\nRevealed in 2021, CLIP (Contrastive Language–Image Pre-training) is a model that is trained to analyze the semantic similarity between text and images. It can notably be used for image classification.\\n\\n\\n=== Text-to-image ===\\n\\n\\n==== DALL-E ====\\nRevealed in 2021, DALL-E is a Transformer model that creates images from textual descriptions. DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a green leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate corresponding images. It can create images of realistic objects (\"a stained-glass window with an image of a blue strawberry\") as well as objects that do not exist in reality (\"a cube with the texture of a porcupine\"). As of March 2021, no API or code is available.\\n\\n\\n===== DALL-E 2 =====\\nIn April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results. In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.\\n\\n\\n===== DALL-E 3 =====\\nIn September 2023, OpenAI announced DALL-E 3, a more powerful model better able to generate images from complex descriptions without manual prompt engineering and render complex details like hands and text. It was released to the public as a ChatGPT Plus feature in October.\\n\\n\\n=== Text-to-video ===\\n\\n\\n==== Sora ====\\n\\nSora is a text-to-video model that can generate videos based on short descriptive prompts as well as extend existing videos forwards or backwards in time. It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\\nSora\\'s development team named it after the Japanese word for \"sky\", to signify its \"limitless creative potential\". Sora\\'s technology is an adaptation of the technology behind the DALL·E 3 text-to-image model. OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for that purpose, but did not reveal the number or the exact sources of the videos.OpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024, stating that it could generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model, and the model\\'s capabilities. It acknowledged some of its shortcomings, including struggles simulating complex physics. Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and might not represent Sora\\'s typical output.Despite skepticism from some academic leaders following Sora\\'s public demo, notable entertainment-industry figures have shown significant interest in the technology\\'s potential. In an interview, actor/filmmaker Tyler Perry expressed his astonishment at the technology\\'s ability to generate realistic video from text descriptions, citing its potential to revolutionize storytelling and content creation. He said that his excitement about Sora\\'s possibilities was so strong that he had decided to pause plans for expanding his Atlanta-based movie studio.\\n\\n\\n=== Speech-to-text ===\\n\\n\\n==== Whisper ====\\n\\nReleased in 2022, Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\\n\\n\\n=== Music generation ===\\n\\n\\n==== MuseNet ====\\nReleased in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with 10 instruments in 15 styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays. In pop culture, initial applications of this tool were used as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character.\\n\\n\\n==== Jukebox ====\\nReleased in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs \"show local musical coherence [and] follow traditional chord patterns\" but acknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and that \"there is a significant gap\" between Jukebox and human-generated music. The Verge stated \"It\\'s technologically impressive, even if the results sound like mushy versions of songs that might feel familiar\", while Business Insider stated \"surprisingly, some of the resulting songs are catchy and sound legitimate\".\\n\\n\\n=== User interfaces ===\\n\\n\\n==== Debate Game ====\\nIn 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.\\n\\n\\n==== Microscope ====\\nReleased in 2020, Microscope is a collection of visualizations of every significant layer and neuron of eight neural network models which are often studied in interpretability. Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG 19, different versions of Inception, and different versions of CLIP Resnet.\\n\\n\\n==== ChatGPT ====\\n\\nLaunched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that provides a conversational interface that allows users to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its launch.As of 2023, ChatGPT Plus is a GPT-4 backed version of ChatGPT available for a US$20 per month subscription fee (the original version is backed by GPT-3.5). OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist; after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices are doubled.In May 2023, OpenAI launched a user interface for ChatGPT for the App Store on iOS and later in July 2023 for the Play Store on Android. The app supports chat history syncing and voice input (using Whisper, OpenAI\\'s speech recognition model). In September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.In October 2023, OpenAI\\'s latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.OpenAI\\'s GPT Store, initially slated for a 2023 launch, is now deferred to an undisclosed date in early 2024, attributed likely to the leadership changes in November following the initial announcement.\\n\\n\\n== Controversies ==\\nIn January 2023, OpenAI has been criticized for outsourcing the annotation of data sets including toxic content to Sama, a company based in San Francisco but employing workers in Kenya. These annotations were used to train an AI model to detect toxicity, which could then be used to filter out toxic content, notably from ChatGPT\\'s training data and outputs. But these pieces of text usually contained detailed descriptions of various types of violence, including sexual violence. The four Sama employees interviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of work, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its annotators. Sama\\'s spokesperson said that the $12.50 was also covering other implicit costs, among which were infrastructure expenses, quality assurance and management.In March 2023, the company was also criticized for disclosing particularly few technical details about products like GPT-4, contradicting its initial commitment to openness and making it harder for independent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and safety concerns to justify this strategic turn. OpenAI\\'s chief scientist Ilya Sutskever argued in 2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons for not open-sourcing the most potent AI models would become \"obvious\" in a few years.OpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul Tremblay and Mona Awad in July 2023. In September 2023, 17 authors, including George R. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class action lawsuit against OpenAI, alleging that the company\\'s technology was illegally using their copyrighted work. The New York Times also sued the company in late December 2023.OpenAI was sued for violating EU General Data Protection Regulations in August 2023. In April 2023, the EU\\'s European Data Protection Board (EDPB) formed a dedicated task force on ChatGPT \"to foster cooperation and to exchange information on possible enforcement actions conducted by data protection authorities\" based on the \"enforcement action undertaken by the Italian data protection authority against Open AI about the Chat GPT service\".On February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against OpenAI on copyright litigation ground. The lawsuit is said to have charted a new legal strategy for digital only publishers to sue OpenAI.\\n\\n\\n=== Removal of military and warfare clause ===\\nOpenAI quietly deleted its ban on using ChatGPT for \"military and warfare\". Up until January 10, 2024, its \"usage policies\" included a ban on \"activity that has high risk of physical harm, including,\" specifically, \"weapons development\" and \"military and warfare.\" Its new policies prohibit \"[using] our service to harm yourself or others\" and to \"develop or use weapons\". As one of the industry collaborators, OpenAI provides LLM to the Artificial Intelligence Cyber Challenge (AIxCC) sponsored by Defense Advanced Research Projects Agency and Advanced Research Projects Agency for Health to protect software critical to Americans.\\n\\n\\n== See also ==\\nAnthropic\\nCenter for AI Safety\\nFuture of Humanity Institute\\nFuture of Life Institute\\nGoogle DeepMind\\nMachine Intelligence Research Institute\\nMicrosoft\\nBing\\nThe New York Times\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website \\nOpenAI on Twitter \\n\"What OpenAI Really Wants\" by Wired\\n\"The Inside Story of Microsoft\\'s Partnership with OpenAI\" by The New Yorker', metadata={'source': '../dataset/llamaindex_data/openai.txt'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "laoder = PyPDFLoader('../dataset/llamaindex_data/Mistral AI - Wikipedia.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".\\nAs one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\\n\\n\\n== History ==\\n\\n\\n=== 2015–2018: Non-profit beginnings ===\\nIn December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Research announced the formation of OpenAI and pledged over $1 billion to the venture. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, Musk was its largest donor while YC Research did not contribute anything at all. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI is headquartered at the Pioneer Building in Mission District, San Francisco.According to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.Microsoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.In April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours. In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI\\'s general intelligence across the world\\'s supply of games, websites, and other applications.In 2017 OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone. In comparison, DeepMind\\'s total expenses in 2017 were $442 million. In the summer of 2018, simply training OpenAI\\'s Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.\\nIn 2018, Musk resigned from his Board of Directors seat, citing \"a potential future conflict [of interest]\" with his role as CEO of Tesla due to Tesla\\'s AI development for self-driving cars. Sam Altman claims that Musk believed OpenAI had fallen behind other players like Google and Musk proposed instead to take over OpenAI himself, which the board rejected. Musk subsequently left OpenAI but claimed to remain a donor, yet made no donations after his departure.In February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like text.\\n\\n\\n=== 2019: Transition from non-profit ===\\nIn 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook , which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.OpenAI Global LLC then announced its intention to commercially license its technologies. It planned to spend the $1 billion \"within five years, and possibly much faster.\" Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated \"I disagree with the notion that a nonprofit can\\'t compete\" and pointed to successful low-budget projects by OpenAI and others. \"If bigger and better funded was always better, then IBM would still be number one.\"\\nThe nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.\\'s nonprofit charter. A majority of OpenAI, Inc.\\'s board is barred from having financial stakes in OpenAI Global LLC. In addition, minority members with a stake in OpenAI Global LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global LLC\\'s switch to for-profit status is inconsistent with OpenAI\\'s claims to be \"democratizing\" AI.\\n\\n\\n=== 2020–2023: ChatGPT, DALL-E, partnership with Microsoft ===\\nIn 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply \"the API\", would form the heart of its first commercial product.In 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex digital images from textual descriptions, utilizing a variant of the GPT-3 architecture.In December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days. According to anonymous sources cited by Reuters in December 2022, OpenAI Global LLC was projecting $200 million of revenue in 2023 and $1 billion in revenue in 2024.In January 2023, OpenAI Global LLC was in talks for funding that would value the company at $29 billion, double its 2021 value. On January 23, 2023, Microsoft announced a new US$10 billion investment in OpenAI Global LLC over multiple years, partially needed to use Microsoft\\'s cloud-computing service Azure. Rumors of this deal suggested that Microsoft may receive 75% of OpenAI\\'s profits until it secures its investment return and a 49% stake in the company. The investment is believed to be a part of Microsoft\\'s efforts to integrate OpenAI\\'s ChatGPT into the Bing search engine. Google announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google\\'s place as a go-to source for information.On February 7, 2023, Microsoft announced that it was building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.On March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest with his investments in AI companies via Greylock Partners, and his co-founding of the AI startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.On March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.On May 22, 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the governance of superintelligence. They consider that superintelligence could happen within the next 10 years, allowing a \"dramatically more prosperous future\" and that \"given the possibility of existential risk, we can\\'t just be reactive\". They propose creating an international watchdog organization similar to IAEA to oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on the other side should not be overly regulated. They also call for more technical safety research for superintelligences, and ask for more coordination, for example through governments launching a joint project which \"many current efforts become part of\".In August 2023, it was announced that OpenAI had acquired the New York-based start-up, Global Illumination, a company that deploys AI to develop digital infrastructure and creative tools.On September 21, 2023, Microsoft began rebranding all variants of its Copilot to Microsoft Copilot, including the former Bing Chat and the Microsoft 365 Copilot. This strategy was followed in December 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well as a standalone Microsoft Copilot app released for Android and one released for iOS thereafter.In October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI would let G42 deploy Open AI technology.On November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of ChatGPT for specific purposes, further expanding the possibilities of AI applications across various industries. On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for ChatGPT Plus due to high demand. Access for newer subscribers re-opened a month later on December 13.\\n\\n\\n==== Brief departure of Altman and Brockman ====\\n\\nOn November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D\\'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company\\'s presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher Szymon Sidor.On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman\\'s departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn\\'t work out. The board members agreed \"in principle\" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO. The board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be joining Microsoft to lead a new advanced AI research team, but added that they were still committed to OpenAI despite recent events. Before the partnership with Microsoft was finalized, Altman gave the board another opportunity to negotiate with him. About 738 of OpenAI\\'s 770 employees, including Murati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the board did not rehire Altman and then resign. This prompted OpenAI investors to consider legal action against the board as well. In response, OpenAI management sent an internal memo to employees stating that negotiations with Altman and the board had resumed and would take some time.\\nOn November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in their prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and Lawrence Summers, with D\\'Angelo remaining. On November 22, 2023, emerging reports suggested that Sam Altman\\'s dismissal from OpenAI may have been linked to his alleged mishandling of a significant breakthrough in the organization\\'s secretive project codenamed Q*. According to sources within OpenAI, Q* is aimed at developing AI capabilities in logical and mathematical reasoning, and reportedly involves performing math on the level of grade-school students. Concerns about Altman\\'s response to this development, specifically regarding the discovery\\'s potential safety implications, were reportedly raised with the company\\'s board shortly before Altman\\'s firing. On November 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as a non-voting member to observe the company\\'s operations.In February 2024, the U.S. Securities and Exchange Commission was reportedly investigating OpenAI over whether internal company communications made by Altman were used to mislead investors; and an investigation of Altman\\'s statements, opened by the Southern New York U.S. Attorney\\'s Office the previous November, was ongoing.\\n\\n\\n=== 2024–present: Public/non-profit efforts, Sora ===\\nOn January 16, 2024, in response to intense scrutiny from regulators around the world, OpenAI announced the formation of a new Collective Alignment team that would aim to implement ideas from the public for ensuring its models would \"align to the values of humanity.\" The move was from its public program launched in May 2023. The company explained that the program would be separate from its commercial endeavors. On January 18, 2024, OpenAI announced a partnership with Arizona State University that would give it complete access to ChatGPT Enterprise. ASU plans to incorporate the technology into various aspects of its operations, including courses, tutoring and research. It is OpenAI\\'s first partnership with an educational institution.On February 15, 2024, OpenAI announced a text-to-video model named Sora, which it plans to release to the public at an unspecified date. It is currently available for red teams for managing critical harms and risks.On February 29, 2024, OpenAI and CEO Sam Altman were sued by Elon Musk, who accused them of prioritizing profits over public good, contrary to OpenAI\\'s original mission of developing AI for humanity\\'s benefit. The lawsuit cited OpenAI\\'s policy shift after partnering with Microsoft, questioning its open-source commitment and stirring the AI ethics-vs.-profit debate. In a blog post, OpenAI stated that \"Elon understood the mission did not imply open-sourcing AGI.\" In a staff memo, they also denied being a de facto Microsoft subsidiary.In a March 11, 2024, court filing, OpenAI said it was \"doing just fine without Elon Musk\" after he left the company in 2018. They also responded to Musk\\'s lawsuit, calling the billionaire’s claims \"incoherent\", \"frivolous\", \"extraordinary\" and \"a fiction\".\\n\\n\\n== Participants ==\\nKey employees\\n\\nCEO and co-founder: Sam Altman, former president of the startup accelerator Y Combinator\\nPresident and co-founder: Greg Brockman, former CTO, 3rd employee of Stripe\\nChief Scientist and co-founder: Ilya Sutskever, a former Google expert on machine learning\\nChief Technology Officer: Mira Murati, previously at Leap Motion and Tesla, Inc.\\nChief Operating Officer: Brad Lightcap, previously at Y Combinator and JPMorgan Chase\\n\\n\\n=== Board of Directors of the OpenAI nonprofit ===\\nMicrosoft (observer)\\nBret Taylor (chairman)\\nSam Altman\\nLawrence Summers\\nAdam D\\'Angelo\\nSue Desmond-Hellmann\\nNicole Seligman\\nFidji Simo\\n\\n\\n=== Principal individual investors ===\\nReid Hoffman, LinkedIn co-founder\\nPeter Thiel, PayPal co-founder\\nJessica Livingston, a founding partner of Y Combinator\\nElon Musk, co-founder\\n\\n\\n=== Corporate investors ===\\nMicrosoft\\nKhosla Ventures\\nInfosys\\nThrive Capital\\n\\n\\n== Motives ==\\nSome scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI gains the ability to redesign itself at an ever-increasing rate, an unstoppable \"intelligence explosion\" could lead to human extinction. Co-founder Musk characterizes AI as humanity\\'s \"biggest existential threat\".Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence. OpenAI states that \"it\\'s hard to fathom how much human-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". Research on safety cannot safely be postponed: \"because of AI\\'s surprising history, it\\'s hard to predict when human-level AI might come within reach.\" OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible.\" Co-chair Sam Altman expects the decades-long project to surpass human intelligence.Vishal Sikka, former CEO of Infosys, stated that an \"openness\", where the endeavor would \"produce results generally in the greater interest of humanity\", was a fundamental requirement for his support; and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\". Cade Metz of Wired suggested that corporations such as Amazon might be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook, which own enormous supplies of proprietary data. Altman stated that Y Combinator companies would share their data with OpenAI.\\n\\n\\n== Strategy ==\\nIn the early years before his 2018 departure, Musk posed the question: \"What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" He acknowledged that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; but nonetheless, that the best defense was \"to empower as many people as possible to have AI. If everyone has AI powers, then there\\'s not any one person or a small set of individuals who can have AI superpower.\"Musk and Altman\\'s counterintuitive strategy—that of trying to reduce of harm from AI by giving everyone access to it—is controversial among those concerned with existential risk from AI. Philosopher Nick Bostrom said, \"If you have a button that could do bad things to the world, you don\\'t want to give it to everyone.\" During a 2016 conversation about technological singularity, Altman said, \"We don\\'t plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated, \"Our goal right now... is to do the best thing there is to do. It\\'s a little vague.\"Conversely, OpenAI\\'s initial decision to withhold GPT-2 around 2019, due to a wish to \"err on the side of caution\" in the presence of potential misuse, was criticized by advocates of openness. Delip Rao, an expert in text generation, stated, \"I don\\'t think [OpenAI] spent enough time proving [GPT-2] was actually dangerous.\" Other critics argued that open publication was necessary to replicate the research and to create countermeasures.More recently, in 2022, OpenAI published its approach to the alignment problem, anticipating that aligning AGI to human values would likely be harder than aligning current AI systems: \"Unaligned AGI could pose substantial risks to humanity[,] and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together\". They stated that they intended to explore how to better use human feedback to train AI systems, and how to safely use AI to incrementally automate alignment research. Some observers believe the company\\'s November 2023 reorganization—including Altman\\'s return as CEO, and the changes to its board of directors—indicated a probable shift towards a business focus and reduced influence of \"cautious people\" at OpenAI.\\n\\n\\n== Products and applications ==\\n\\n\\n=== Reinforcement learning ===\\nAt its beginning, OpenAI\\'s research included many projects focused on reinforcement learning (RL). OpenAI has been viewed as an important competitor to DeepMind.\\n\\n\\n==== Gym ====\\nAnnounced in 2016, Gym aimed to provide an easily implemented general-intelligence benchmark in a wide variety of environments—akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research. It sought to standardize how environments were defined in AI research publications, so that published research became more easily reproducible, and to provide users with a simple interface. As of June 2017, Gym could be used only with Python. As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its GitHub page.\\n\\n\\n==== Gym Retro ====\\nReleased in 2018, Gym Retro is a platform for reinforcement learning (RL) research on video games, using RL algorithms and study generalization. Prior RL research focused mainly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.\\n\\n\\n==== RoboSumo ====\\nReleased in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack knowledge of how to even walk, but are given the goals of learning to move and to push the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions. When an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way. OpenAI\\'s Igor Mordatch argued that competition between agents could create an intelligence \"arms race\" that could increase an agent\\'s ability to function even outside the context of the competition.\\n\\n\\n==== OpenAI Five ====\\n\\nOpenAI Five is a team of five OpenAI-curated bots used in the competitive five-on-five video game Dota 2, that learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon. The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.By June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games. In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco. The bots\\' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.OpenAI Five\\'s mechanisms in Dota 2\\'s bot player shows the challenges of AI systems in multiplayer online battle arena (MOBA) games and how OpenAI Five has demonstrated the use of deep reinforcement learning (DRL) agents to achieve superhuman competence in Dota 2 matches.\\n\\n\\n==== Dactyl ====\\nDeveloped in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects. It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.In 2019, OpenAI demonstrated that Dactyl could solve a Rubik\\'s Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik\\'s Cube introduce complex physics that is harder to model. OpenAI did this by improving the robustness of Dactyl to perturbations by using Automatic Domain Randomization (ADR), a simulation approach of generating progressively more difficult environments. ADR differs from manual domain randomization by not needing a human to specify randomization ranges.\\n\\n\\n=== API ===\\nIn June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task\".\\n\\n\\n=== Text generation ===\\nThe company has popularized generative pretrained transformers (GPT).\\n\\n\\n==== OpenAI\\'s original GPT model (\"GPT-1\") ====\\n\\nThe original paper on generative pre-training of a transformer-based language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI\\'s website on June 11, 2018. It showed how a generative model of language could acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.\\n\\n\\n==== GPT-2 ====\\n\\nGenerative Pre-trained Transformer 2 (\"GPT-2\") is an unsupervised transformer language model and the successor to OpenAI\\'s original GPT model (\"GPT-1\"). GPT-2 was announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released due to concern about potential misuse, including applications for writing fake news. Some experts expressed skepticism that GPT-2 posed a significant threat.\\nIn response to GPT-2, the Allen Institute for Artificial Intelligence responded with a tool to detect \"neural fake news\". Other researchers, such as Jeremy Howard, warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\". In November 2019, OpenAI released the complete version of the GPT-2 language model. Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.GPT-2\\'s authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples).\\nThe corpus it was trained on, called WebText, contains slightly 40 gigabytes of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This permits representing any string of characters by encoding both individual characters and multiple-character tokens.\\n\\n\\n==== GPT-3 ====\\n\\nFirst described in May 2020, Generative Pre-trained Transformer 3 (GPT-3) is an unsupervised transformer language model and the successor to GPT-2. OpenAI stated that the full version of GPT-3 contained 175 billion parameters, two orders of magnitude larger than the 1.5 billion in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).OpenAI stated that GPT-3 succeeded at certain \"meta-learning\" tasks and could generalize the purpose of a single input-output pair. The GPT-3 release paper gave examples of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.GPT-3 dramatically improved benchmark results  over GPT-2. OpenAI cautioned that such scaling-up of language models could be approaching or encountering the fundamental capability limitations of predictive language models. Pre-training GPT-3 required several thousand petaflop/s-days of compute, compared to tens of petaflop/s-days for the full GPT-2 model. Like its predecessor, the GPT-3 trained model was not immediately released to the public for concerns of possible abuse, although OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.\\n\\n\\n==== Codex ====\\n\\nAnnounced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories, and is the AI powering the code autocompletion tool GitHub Copilot. In August 2021, an API was released in private beta. According to OpenAI, the model can create working code in over a dozen programming languages, most effectively in Python.Several issues with glitches, design flaws and security vulnerabilities were cited.GitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.OpenAI announced that they would discontinue support for Codex API on March 23, 2023.\\n\\n\\n==== GPT-4 ====\\n\\nOn March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs. They announced that the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5 scored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n\\n=== Image classification ===\\n\\n\\n==== CLIP ====\\nRevealed in 2021, CLIP (Contrastive Language–Image Pre-training) is a model that is trained to analyze the semantic similarity between text and images. It can notably be used for image classification.\\n\\n\\n=== Text-to-image ===\\n\\n\\n==== DALL-E ====\\nRevealed in 2021, DALL-E is a Transformer model that creates images from textual descriptions. DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a green leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate corresponding images. It can create images of realistic objects (\"a stained-glass window with an image of a blue strawberry\") as well as objects that do not exist in reality (\"a cube with the texture of a porcupine\"). As of March 2021, no API or code is available.\\n\\n\\n===== DALL-E 2 =====\\nIn April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results. In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.\\n\\n\\n===== DALL-E 3 =====\\nIn September 2023, OpenAI announced DALL-E 3, a more powerful model better able to generate images from complex descriptions without manual prompt engineering and render complex details like hands and text. It was released to the public as a ChatGPT Plus feature in October.\\n\\n\\n=== Text-to-video ===\\n\\n\\n==== Sora ====\\n\\nSora is a text-to-video model that can generate videos based on short descriptive prompts as well as extend existing videos forwards or backwards in time. It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\\nSora\\'s development team named it after the Japanese word for \"sky\", to signify its \"limitless creative potential\". Sora\\'s technology is an adaptation of the technology behind the DALL·E 3 text-to-image model. OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for that purpose, but did not reveal the number or the exact sources of the videos.OpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024, stating that it could generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model, and the model\\'s capabilities. It acknowledged some of its shortcomings, including struggles simulating complex physics. Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and might not represent Sora\\'s typical output.Despite skepticism from some academic leaders following Sora\\'s public demo, notable entertainment-industry figures have shown significant interest in the technology\\'s potential. In an interview, actor/filmmaker Tyler Perry expressed his astonishment at the technology\\'s ability to generate realistic video from text descriptions, citing its potential to revolutionize storytelling and content creation. He said that his excitement about Sora\\'s possibilities was so strong that he had decided to pause plans for expanding his Atlanta-based movie studio.\\n\\n\\n=== Speech-to-text ===\\n\\n\\n==== Whisper ====\\n\\nReleased in 2022, Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\\n\\n\\n=== Music generation ===\\n\\n\\n==== MuseNet ====\\nReleased in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with 10 instruments in 15 styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays. In pop culture, initial applications of this tool were used as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character.\\n\\n\\n==== Jukebox ====\\nReleased in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs \"show local musical coherence [and] follow traditional chord patterns\" but acknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and that \"there is a significant gap\" between Jukebox and human-generated music. The Verge stated \"It\\'s technologically impressive, even if the results sound like mushy versions of songs that might feel familiar\", while Business Insider stated \"surprisingly, some of the resulting songs are catchy and sound legitimate\".\\n\\n\\n=== User interfaces ===\\n\\n\\n==== Debate Game ====\\nIn 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.\\n\\n\\n==== Microscope ====\\nReleased in 2020, Microscope is a collection of visualizations of every significant layer and neuron of eight neural network models which are often studied in interpretability. Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG 19, different versions of Inception, and different versions of CLIP Resnet.\\n\\n\\n==== ChatGPT ====\\n\\nLaunched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that provides a conversational interface that allows users to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its launch.As of 2023, ChatGPT Plus is a GPT-4 backed version of ChatGPT available for a US$20 per month subscription fee (the original version is backed by GPT-3.5). OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist; after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices are doubled.In May 2023, OpenAI launched a user interface for ChatGPT for the App Store on iOS and later in July 2023 for the Play Store on Android. The app supports chat history syncing and voice input (using Whisper, OpenAI\\'s speech recognition model). In September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.In October 2023, OpenAI\\'s latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.OpenAI\\'s GPT Store, initially slated for a 2023 launch, is now deferred to an undisclosed date in early 2024, attributed likely to the leadership changes in November following the initial announcement.\\n\\n\\n== Controversies ==\\nIn January 2023, OpenAI has been criticized for outsourcing the annotation of data sets including toxic content to Sama, a company based in San Francisco but employing workers in Kenya. These annotations were used to train an AI model to detect toxicity, which could then be used to filter out toxic content, notably from ChatGPT\\'s training data and outputs. But these pieces of text usually contained detailed descriptions of various types of violence, including sexual violence. The four Sama employees interviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of work, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its annotators. Sama\\'s spokesperson said that the $12.50 was also covering other implicit costs, among which were infrastructure expenses, quality assurance and management.In March 2023, the company was also criticized for disclosing particularly few technical details about products like GPT-4, contradicting its initial commitment to openness and making it harder for independent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and safety concerns to justify this strategic turn. OpenAI\\'s chief scientist Ilya Sutskever argued in 2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons for not open-sourcing the most potent AI models would become \"obvious\" in a few years.OpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul Tremblay and Mona Awad in July 2023. In September 2023, 17 authors, including George R. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class action lawsuit against OpenAI, alleging that the company\\'s technology was illegally using their copyrighted work. The New York Times also sued the company in late December 2023.OpenAI was sued for violating EU General Data Protection Regulations in August 2023. In April 2023, the EU\\'s European Data Protection Board (EDPB) formed a dedicated task force on ChatGPT \"to foster cooperation and to exchange information on possible enforcement actions conducted by data protection authorities\" based on the \"enforcement action undertaken by the Italian data protection authority against Open AI about the Chat GPT service\".On February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against OpenAI on copyright litigation ground. The lawsuit is said to have charted a new legal strategy for digital only publishers to sue OpenAI.\\n\\n\\n=== Removal of military and warfare clause ===\\nOpenAI quietly deleted its ban on using ChatGPT for \"military and warfare\". Up until January 10, 2024, its \"usage policies\" included a ban on \"activity that has high risk of physical harm, including,\" specifically, \"weapons development\" and \"military and warfare.\" Its new policies prohibit \"[using] our service to harm yourself or others\" and to \"develop or use weapons\". As one of the industry collaborators, OpenAI provides LLM to the Artificial Intelligence Cyber Challenge (AIxCC) sponsored by Defense Advanced Research Projects Agency and Advanced Research Projects Agency for Health to protect software critical to Americans.\\n\\n\\n== See also ==\\nAnthropic\\nCenter for AI Safety\\nFuture of Humanity Institute\\nFuture of Life Institute\\nGoogle DeepMind\\nMachine Intelligence Research Institute\\nMicrosoft\\nBing\\nThe New York Times\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website \\nOpenAI on Twitter \\n\"What OpenAI Really Wants\" by Wired\\n\"The Inside Story of Microsoft\\'s Partnership with OpenAI\" by The New Yorker', metadata={'source': '../dataset/llamaindex_data/openai.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf의 경우 여러 페이지는 여러 개의 문서로 취급된다.\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:31<00:31, 31.98s/it]This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2/2 [01:06<00:00, 33.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# 특정 디렉토리에 있는 모든 문서를 load하는 방법도 존재한다.\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# glob 옵션을 사용하면 파일을 필터링할 수 있다.\n",
    "loader = DirectoryLoader('../dataset/llamaindex_data', glob='*', show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned.\\n\\nThe new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\\n\\n== History ==\\n\\n=== 2015–2018: Non-profit beginnings === In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Research announced the formation of OpenAI and pledged over $1 billion to the venture. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, Musk was its largest donor while YC Research did not contribute anything at all. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI is headquartered at the Pioneer Building in Mission District, San Francisco.According to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.Microsoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\"\\n\\nBrockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.In April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours. In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI\\'s general intelligence across the world\\'s supply of games, websites, and other applications.In 2017 OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone. In comparison, DeepMind\\'s total expenses in 2017 were $442 million. In the summer of 2018, simply training OpenAI\\'s Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks. In 2018, Musk resigned from his Board of Directors seat, citing \"a potential future conflict [of interest]\" with his role as CEO of Tesla due to Tesla\\'s AI development for self-driving cars. Sam Altman claims that Musk believed OpenAI had fallen behind other players like Google and Musk proposed instead to take over OpenAI himself, which the board rejected.\\n\\nMusk subsequently left OpenAI but claimed to remain a donor, yet made no donations after his departure.In February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like text.\\n\\n=== 2019: Transition from non-profit === In 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook , which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.OpenAI Global LLC then announced its intention to commercially license its technologies. It planned to spend the $1 billion \"within five years, and possibly much faster.\"\\n\\nAltman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated \"I disagree with the notion that a nonprofit can\\'t compete\" and pointed to successful low-budget projects by OpenAI and others. \"If bigger and better funded was always better, then IBM would still be number one.\" The nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.\\'s nonprofit charter. A majority of OpenAI, Inc.\\'s board is barred from having financial stakes in OpenAI Global LLC. In addition, minority members with a stake in OpenAI Global LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global LLC\\'s switch to for-profit status is inconsistent with OpenAI\\'s claims to be \"democratizing\" AI.\\n\\n=== 2020–2023: ChatGPT, DALL-E, partnership with Microsoft === In 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply \"the API\", would form the heart of its first commercial product.In 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex digital images from textual descriptions, utilizing a variant of the GPT-3 architecture.In December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days. According to anonymous sources cited by Reuters in December 2022, OpenAI Global LLC was projecting $200 million of revenue in 2023 and $1 billion in revenue in 2024.In January 2023, OpenAI Global LLC was in talks for funding that would value the company at $29 billion, double its 2021 value. On January 23, 2023, Microsoft announced a new US$10 billion investment in OpenAI Global LLC over multiple years, partially needed to use Microsoft\\'s cloud-computing service Azure. Rumors of this deal suggested that Microsoft may receive 75% of OpenAI\\'s profits until it secures its investment return and a 49% stake in the company.\\n\\nThe investment is believed to be a part of Microsoft\\'s efforts to integrate OpenAI\\'s ChatGPT into the Bing search engine. Google announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google\\'s place as a go-to source for information.On February 7, 2023, Microsoft announced that it was building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.On March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest with his investments in AI companies via Greylock Partners, and his co-founding of the AI startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.On March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.On May 22, 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the governance of superintelligence. They consider that superintelligence could happen within the next 10 years, allowing a \"dramatically more prosperous future\" and that \"given the possibility of existential risk, we can\\'t just be reactive\". They propose creating an international watchdog organization similar to IAEA to oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on the other side should not be overly regulated.\\n\\nThey also call for more technical safety research for superintelligences, and ask for more coordination, for example through governments launching a joint project which \"many current efforts become part of\".In August 2023, it was announced that OpenAI had acquired the New York-based start-up, Global Illumination, a company that deploys AI to develop digital infrastructure and creative tools.On September 21, 2023, Microsoft began rebranding all variants of its Copilot to Microsoft Copilot, including the former Bing Chat and the Microsoft 365 Copilot. This strategy was followed in December 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well as a standalone Microsoft Copilot app released for Android and one released for iOS thereafter.In October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI would let G42 deploy Open AI technology.On November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of ChatGPT for specific purposes, further expanding the possibilities of AI applications across various industries. On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for ChatGPT Plus due to high demand. Access for newer subscribers re-opened a month later on December 13.\\n\\n==== Brief departure of Altman and Brockman ====\\n\\nOn November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D\\'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company\\'s presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher Szymon Sidor.On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman\\'s departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn\\'t work out. The board members agreed \"in principle\" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO.\\n\\nThe board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be joining Microsoft to lead a new advanced AI research team, but added that they were still committed to OpenAI despite recent events. Before the partnership with Microsoft was finalized, Altman gave the board another opportunity to negotiate with him. About 738 of OpenAI\\'s 770 employees, including Murati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the board did not rehire Altman and then resign. This prompted OpenAI investors to consider legal action against the board as well. In response, OpenAI management sent an internal memo to employees stating that negotiations with Altman and the board had resumed and would take some time. On November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in their prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and Lawrence Summers, with D\\'Angelo remaining. On November 22, 2023, emerging reports suggested that Sam Altman\\'s dismissal from OpenAI may have been linked to his alleged mishandling of a significant breakthrough in the organization\\'s secretive project codenamed Q*.\\n\\nAccording to sources within OpenAI, Q* is aimed at developing AI capabilities in logical and mathematical reasoning, and reportedly involves performing math on the level of grade-school students. Concerns about Altman\\'s response to this development, specifically regarding the discovery\\'s potential safety implications, were reportedly raised with the company\\'s board shortly before Altman\\'s firing. On November 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as a non-voting member to observe the company\\'s operations.In February 2024, the U.S. Securities and Exchange Commission was reportedly investigating OpenAI over whether internal company communications made by Altman were used to mislead investors; and an investigation of Altman\\'s statements, opened by the Southern New York U.S. Attorney\\'s Office the previous November, was ongoing.\\n\\n=== 2024–present: Public/non-profit efforts, Sora === On January 16, 2024, in response to intense scrutiny from regulators around the world, OpenAI announced the formation of a new Collective Alignment team that would aim to implement ideas from the public for ensuring its models would \"align to the values of humanity.\" The move was from its public program launched in May 2023. The company explained that the program would be separate from its commercial endeavors. On January 18, 2024, OpenAI announced a partnership with Arizona State University that would give it complete access to ChatGPT Enterprise. ASU plans to incorporate the technology into various aspects of its operations, including courses, tutoring and research. It is OpenAI\\'s first partnership with an educational institution.On February 15, 2024, OpenAI announced a text-to-video model named Sora, which it plans to release to the public at an unspecified date. It is currently available for red teams for managing critical harms and risks.On February 29, 2024, OpenAI and CEO Sam Altman were sued by Elon Musk, who accused them of prioritizing profits over public good, contrary to OpenAI\\'s original mission of developing AI for humanity\\'s benefit. The lawsuit cited OpenAI\\'s policy shift after partnering with Microsoft, questioning its open-source commitment and stirring the AI ethics-vs.-profit debate. In a blog post, OpenAI stated that \"Elon understood the mission did not imply open-sourcing AGI.\"\\n\\nIn a staff memo, they also denied being a de facto Microsoft subsidiary.In a March 11, 2024, court filing, OpenAI said it was \"doing just fine without Elon Musk\" after he left the company in 2018. They also responded to Musk\\'s lawsuit, calling the billionaire’s claims \"incoherent\", \"frivolous\", \"extraordinary\" and \"a fiction\".\\n\\n== Participants ==\\n\\nKey employees\\n\\nCEO and co-founder: Sam Altman, former president of the startup accelerator Y Combinator President and co-founder: Greg Brockman, former CTO, 3rd employee of Stripe Chief Scientist and co-founder: Ilya Sutskever, a former Google expert on machine learning Chief Technology Officer: Mira Murati, previously at Leap Motion and Tesla, Inc. Chief Operating Officer: Brad Lightcap, previously at Y Combinator and JPMorgan Chase\\n\\n=== Board of Directors of the OpenAI nonprofit === Microsoft (observer) Bret Taylor (chairman) Sam Altman Lawrence Summers Adam D\\'Angelo Sue Desmond-Hellmann Nicole Seligman Fidji Simo\\n\\n=== Principal individual investors === Reid Hoffman, LinkedIn co-founder Peter Thiel, PayPal co-founder Jessica Livingston, a founding partner of Y Combinator Elon Musk, co-founder\\n\\n=== Corporate investors ===\\n\\nMicrosoft\\n\\nKhosla Ventures\\n\\nInfosys\\n\\nThrive Capital\\n\\n== Motives == Some scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI gains the ability to redesign itself at an ever-increasing rate, an unstoppable \"intelligence explosion\" could lead to human extinction. Co-founder Musk characterizes AI as humanity\\'s \"biggest existential threat\".Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence. OpenAI states that \"it\\'s hard to fathom how much human-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". Research on safety cannot safely be postponed: \"because of AI\\'s surprising history, it\\'s hard to predict when human-level AI might come within reach.\" OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible.\" Co-chair Sam Altman expects the decades-long project to surpass human intelligence.Vishal Sikka, former CEO of Infosys, stated that an \"openness\", where the endeavor would \"produce results generally in the greater interest of humanity\", was a fundamental requirement for his support; and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\".\\n\\nCade Metz of Wired suggested that corporations such as Amazon might be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook, which own enormous supplies of proprietary data. Altman stated that Y Combinator companies would share their data with OpenAI.\\n\\n== Strategy == In the early years before his 2018 departure, Musk posed the question: \"What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" He acknowledged that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; but nonetheless, that the best defense was \"to empower as many people as possible to have AI. If everyone has AI powers, then there\\'s not any one person or a small set of individuals who can have AI superpower. \"Musk and Altman\\'s counterintuitive strategy—that of trying to reduce of harm from AI by giving everyone access to it—is controversial among those concerned with existential risk from AI. Philosopher Nick Bostrom said, \"If you have a button that could do bad things to the world, you don\\'t want to give it to everyone.\" During a 2016 conversation about technological singularity, Altman said, \"We don\\'t plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated, \"Our goal right now... is to do the best thing there is to do. It\\'s a little vague.\\n\\n\"Conversely, OpenAI\\'s initial decision to withhold GPT-2 around 2019, due to a wish to \"err on the side of caution\" in the presence of potential misuse, was criticized by advocates of openness. Delip Rao, an expert in text generation, stated, \"I don\\'t think [OpenAI] spent enough time proving [GPT-2] was actually dangerous.\" Other critics argued that open publication was necessary to replicate the research and to create countermeasures.More recently, in 2022, OpenAI published its approach to the alignment problem, anticipating that aligning AGI to human values would likely be harder than aligning current AI systems: \"Unaligned AGI could pose substantial risks to humanity[,] and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together\". They stated that they intended to explore how to better use human feedback to train AI systems, and how to safely use AI to incrementally automate alignment research. Some observers believe the company\\'s November 2023 reorganization—including Altman\\'s return as CEO, and the changes to its board of directors—indicated a probable shift towards a business focus and reduced influence of \"cautious people\" at OpenAI.\\n\\n== Products and applications ==\\n\\n=== Reinforcement learning === At its beginning, OpenAI\\'s research included many projects focused on reinforcement learning (RL). OpenAI has been viewed as an important competitor to DeepMind.\\n\\n==== Gym ==== Announced in 2016, Gym aimed to provide an easily implemented general-intelligence benchmark in a wide variety of environments—akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research. It sought to standardize how environments were defined in AI research publications, so that published research became more easily reproducible, and to provide users with a simple interface. As of June 2017, Gym could be used only with Python. As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its GitHub page.\\n\\n==== Gym Retro ==== Released in 2018, Gym Retro is a platform for reinforcement learning (RL) research on video games, using RL algorithms and study generalization. Prior RL research focused mainly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.\\n\\n==== RoboSumo ==== Released in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack knowledge of how to even walk, but are given the goals of learning to move and to push the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions. When an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way. OpenAI\\'s Igor Mordatch argued that competition between agents could create an intelligence \"arms race\" that could increase an agent\\'s ability to function even outside the context of the competition.\\n\\n==== OpenAI Five ====\\n\\nOpenAI Five is a team of five OpenAI-curated bots used in the competitive five-on-five video game Dota 2, that learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon. The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.By June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games. In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco.\\n\\nThe bots\\' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.OpenAI Five\\'s mechanisms in Dota 2\\'s bot player shows the challenges of AI systems in multiplayer online battle arena (MOBA) games and how OpenAI Five has demonstrated the use of deep reinforcement learning (DRL) agents to achieve superhuman competence in Dota 2 matches.\\n\\n==== Dactyl ==== Developed in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects. It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.In 2019, OpenAI demonstrated that Dactyl could solve a Rubik\\'s Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik\\'s Cube introduce complex physics that is harder to model. OpenAI did this by improving the robustness of Dactyl to perturbations by using Automatic Domain Randomization (ADR), a simulation approach of generating progressively more difficult environments. ADR differs from manual domain randomization by not needing a human to specify randomization ranges.\\n\\n=== API === In June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task\".\\n\\n=== Text generation === The company has popularized generative pretrained transformers (GPT).\\n\\n==== OpenAI\\'s original GPT model (\"GPT-1\") ====\\n\\nThe original paper on generative pre-training of a transformer-based language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI\\'s website on June 11, 2018. It showed how a generative model of language could acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.\\n\\n==== GPT\\n\\n2 ====\\n\\nGenerative Pre-trained Transformer 2 (\"GPT-2\") is an unsupervised transformer language model and the successor to OpenAI\\'s original GPT model (\"GPT-1\"). GPT-2 was announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released due to concern about potential misuse, including applications for writing fake news. Some experts expressed skepticism that GPT-2 posed a significant threat. In response to GPT-2, the Allen Institute for Artificial Intelligence responded with a tool to detect \"neural fake news\". Other researchers, such as Jeremy Howard, warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\". In November 2019, OpenAI released the complete version of the GPT-2 language model. Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.GPT-2\\'s authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples). The corpus it was trained on, called WebText, contains slightly 40 gigabytes of text from URLs shared in Reddit submissions with at least 3 upvotes.\\n\\nIt avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This permits representing any string of characters by encoding both individual characters and multiple-character tokens.\\n\\n==== GPT\\n\\n3 ====\\n\\nFirst described in May 2020, Generative Pre-trained Transformer 3 (GPT-3) is an unsupervised transformer language model and the successor to GPT-2. OpenAI stated that the full version of GPT-3 contained 175 billion parameters, two orders of magnitude larger than the 1.5 billion in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).OpenAI stated that GPT-3 succeeded at certain \"meta-learning\" tasks and could generalize the purpose of a single input-output pair. The GPT-3 release paper gave examples of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.GPT-3 dramatically improved benchmark results  over GPT-2. OpenAI cautioned that such scaling-up of language models could be approaching or encountering the fundamental capability limitations of predictive language models. Pre-training GPT-3 required several thousand petaflop/s-days of compute, compared to tens of petaflop/s-days for the full GPT-2 model. Like its predecessor, the GPT-3 trained model was not immediately released to the public for concerns of possible abuse, although OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.\\n\\n==== Codex ====\\n\\nAnnounced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories, and is the AI powering the code autocompletion tool GitHub Copilot. In August 2021, an API was released in private beta. According to OpenAI, the model can create working code in over a dozen programming languages, most effectively in Python.Several issues with glitches, design flaws and security vulnerabilities were cited.GitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.OpenAI announced that they would discontinue support for Codex API on March 23, 2023.\\n\\n==== GPT\\n\\n4 ====\\n\\nOn March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs. They announced that the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5 scored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n=== Image classification ===\\n\\n==== CLIP ==== Revealed in 2021, CLIP (Contrastive Language–Image Pre-training) is a model that is trained to analyze the semantic similarity between text and images. It can notably be used for image classification.\\n\\n=== Text\\n\\nto\\n\\nimage ===\\n\\n==== DALL-E ==== Revealed in 2021, DALL-E is a Transformer model that creates images from textual descriptions. DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a green leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate corresponding images. It can create images of realistic objects (\"a stained-glass window with an image of a blue strawberry\") as well as objects that do not exist in reality (\"a cube with the texture of a porcupine\"). As of March 2021, no API or code is available.\\n\\n===== DALL-E 2 ===== In April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results. In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.\\n\\n===== DALL-E 3 ===== In September 2023, OpenAI announced DALL-E 3, a more powerful model better able to generate images from complex descriptions without manual prompt engineering and render complex details like hands and text. It was released to the public as a ChatGPT Plus feature in October.\\n\\n=== Text\\n\\nto\\n\\nvideo ===\\n\\n==== Sora ====\\n\\nSora is a text-to-video model that can generate videos based on short descriptive prompts as well as extend existing videos forwards or backwards in time. It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown. Sora\\'s development team named it after the Japanese word for \"sky\", to signify its \"limitless creative potential\". Sora\\'s technology is an adaptation of the technology behind the DALL·E 3 text-to-image model. OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for that purpose, but did not reveal the number or the exact sources of the videos.OpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024, stating that it could generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model, and the model\\'s capabilities. It acknowledged some of its shortcomings, including struggles simulating complex physics. Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and might not represent Sora\\'s typical output.Despite skepticism from some academic leaders following Sora\\'s public demo, notable entertainment-industry figures have shown significant interest in the technology\\'s potential.\\n\\nIn an interview, actor/filmmaker Tyler Perry expressed his astonishment at the technology\\'s ability to generate realistic video from text descriptions, citing its potential to revolutionize storytelling and content creation. He said that his excitement about Sora\\'s possibilities was so strong that he had decided to pause plans for expanding his Atlanta-based movie studio.\\n\\n=== Speech\\n\\nto\\n\\ntext ===\\n\\n==== Whisper ====\\n\\nReleased in 2022, Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\\n\\n=== Music generation ===\\n\\n==== MuseNet ==== Released in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with 10 instruments in 15 styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays. In pop culture, initial applications of this tool were used as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character.\\n\\n==== Jukebox ==== Released in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs \"show local musical coherence [and] follow traditional chord patterns\" but acknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and that \"there is a significant gap\" between Jukebox and human-generated music. The Verge stated \"It\\'s technologically impressive, even if the results sound like mushy versions of songs that might feel familiar\", while Business Insider stated \"surprisingly, some of the resulting songs are catchy and sound legitimate\".\\n\\n=== User interfaces ===\\n\\n==== Debate Game ==== In 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.\\n\\n==== Microscope ==== Released in 2020, Microscope is a collection of visualizations of every significant layer and neuron of eight neural network models which are often studied in interpretability. Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG 19, different versions of Inception, and different versions of CLIP Resnet.\\n\\n==== ChatGPT ====\\n\\nLaunched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that provides a conversational interface that allows users to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its launch.As of 2023, ChatGPT Plus is a GPT-4 backed version of ChatGPT available for a US$20 per month subscription fee (the original version is backed by GPT-3.5). OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist; after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices are doubled.In May 2023, OpenAI launched a user interface for ChatGPT for the App Store on iOS and later in July 2023 for the Play Store on Android. The app supports chat history syncing and voice input (using Whisper, OpenAI\\'s speech recognition model). In September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.In October 2023, OpenAI\\'s latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise.\\n\\nThe integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.OpenAI\\'s GPT Store, initially slated for a 2023 launch, is now deferred to an undisclosed date in early 2024, attributed likely to the leadership changes in November following the initial announcement.\\n\\n== Controversies == In January 2023, OpenAI has been criticized for outsourcing the annotation of data sets including toxic content to Sama, a company based in San Francisco but employing workers in Kenya. These annotations were used to train an AI model to detect toxicity, which could then be used to filter out toxic content, notably from ChatGPT\\'s training data and outputs. But these pieces of text usually contained detailed descriptions of various types of violence, including sexual violence. The four Sama employees interviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of work, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its annotators. Sama\\'s spokesperson said that the $12.50 was also covering other implicit costs, among which were infrastructure expenses, quality assurance and management.In March 2023, the company was also criticized for disclosing particularly few technical details about products like GPT-4, contradicting its initial commitment to openness and making it harder for independent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and safety concerns to justify this strategic turn.\\n\\nOpenAI\\'s chief scientist Ilya Sutskever argued in 2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons for not open-sourcing the most potent AI models would become \"obvious\" in a few years.OpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul Tremblay and Mona Awad in July 2023. In September 2023, 17 authors, including George R. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class action lawsuit against OpenAI, alleging that the company\\'s technology was illegally using their copyrighted work. The New York Times also sued the company in late December 2023.OpenAI was sued for violating EU General Data Protection Regulations in August 2023. In April 2023, the EU\\'s European Data Protection Board (EDPB) formed a dedicated task force on ChatGPT \"to foster cooperation and to exchange information on possible enforcement actions conducted by data protection authorities\" based on the \"enforcement action undertaken by the Italian data protection authority against Open AI about the Chat GPT service\".On February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against OpenAI on copyright litigation ground. The lawsuit is said to have charted a new legal strategy for digital only publishers to sue OpenAI.\\n\\n=== Removal of military and warfare clause === OpenAI quietly deleted its ban on using ChatGPT for \"military and warfare\". Up until January 10, 2024, its \"usage policies\" included a ban on \"activity that has high risk of physical harm, including,\" specifically, \"weapons development\" and \"military and warfare.\" Its new policies prohibit \"[using] our service to harm yourself or others\" and to \"develop or use weapons\". As one of the industry collaborators, OpenAI provides LLM to the Artificial Intelligence Cyber Challenge (AIxCC) sponsored by Defense Advanced Research Projects Agency and Advanced Research Projects Agency for Health to protect software critical to Americans.\\n\\n== See also ==\\n\\nAnthropic\\n\\nCenter for AI Safety\\n\\nFuture of Humanity Institute\\n\\nFuture of Life Institute\\n\\nGoogle DeepMind\\n\\nMachine Intelligence Research Institute\\n\\nMicrosoft\\n\\nBing\\n\\nThe New York Times\\n\\n== Notes ==\\n\\n== References ==\\n\\n== External links ==\\n\\nOfficial website OpenAI on Twitter \"What OpenAI Really Wants\" by Wired \"The Inside Story of Microsoft\\'s Partnership with OpenAI\" by The New Yorker', metadata={'source': '../dataset/llamaindex_data/openai.txt'}),\n",
       " Document(page_content='24. 4. 10. 오후 3:22\\n\\nMistral AI - Wikipedia\\n\\nWIKIPEDIA\\n\\nThe Free Encyclopedia\\n\\nMistral AI\\n\\nMistral AI is a French company selling artificial intelligence (AI) products. It was founded in April 2023 by previous employees of Meta Platforms and Google DeepMind.[1] The company raised €385 million in October 2023[2] and in December 2023 it was valued at more than $2 billion.[3][4][5]\\n\\nMistral AI\\n\\nCompany type Industry Private Artificial intelligence Founded 28 April 2023 Founders Arthur Mensch (Co-Founder & CEO) Guillaume Lample (Co-Founder & Chief Scientist) Timothée Lacroix (Co-Founder & CTO) Headquarters Paris, France Products Mistral 7B Mixtral 8x7B Mistral Medium Mistral Large mistral.ai (https://mis tral.ai/) Website\\n\\nIt produces open source large language models,[6] citing the foundational importance of open-source software, and as a response to proprietary models.[7]\\n\\nAs of March 2024, two models have been published and are available as weights.[8] Three more models, Small, Medium and Large, are available via API only.[9][10]\\n\\nHistory\\n\\nMistral AI was co-founded in April 2023 by Arthur Mensch, Guillaume Lample and Timothée Lacroix. Prior to co-founding Mistral AI, Arthur Mensch worked at Google DeepMind which is Google\\'s artificial intelligence laboratory, while Guillaume Lample and Timothée Lacroix worked at Meta Platforms.[11] The co-founders met while students at École polytechnique. Mistral is named for a strong wind that blows in France.[12]\\n\\n| |\\n\\nIn June 2023, the start-up carried out a first fundraising of €105 million ($117 million) with investors including the American fund Lightspeed Venture Partners, Eric Schmidt, Xavier Niel and JCDecaux. The valuation is then estimated by the Financial Times at €240 million ($267 million).\\n\\nOn 27 September 2023, the company made its language processing model “Mistral 7B” available under the free Apache 2.0 license. This model has 7 billion parameters, a small size compared to its competitors.\\n\\nOn 10 December 2023, Mistral AI announced that it had raised €385 million ($428 million) as part of its second fundraising. This round of financing notably involves the Californian fund Andreessen Horowitz, BNP Paribas and the software publisher Salesforce.[13]\\n\\nOn 11 December 2023, the company released the “Mixtral 8x7B” model with 46.7 billion parameters but using only 12.9 billion per token thanks to the mixture of experts architecture. The model masters 5 languages (French, Spanish, Italian, English and German) and outperforms, according to its developers\\' tests, the \"LLama 2 70B\" model from Meta. A version trained to follow instructions and called “Mixtral 8x7B Instruct” is also offered.[14]\\n\\nhttps://en.wikipedia.org/wiki/Mistral_AI\\n\\n1/5\\n\\n24. 4. 10. 오후 3:22\\n\\nMistral AI - Wikipedia\\n\\nOn 26 February 2024, Microsoft announced a new partnership with the company to expand its presence in the rapidly evolving artificial intelligence industry. Under the agreement, Mistral\\'s rich language models will be available on Microsoft\\'s Azure cloud, while the multilingual conversational assistant \"Le Chat\" will be launched in the style of ChatGPT.[15]\\n\\nModels\\n\\nOpen Weight Models\\n\\nMistral 7B\\n\\nMistral 7B is a 7.3B parameter language model using the transformers architecture. Officially released on September 27, 2023, via a BitTorrent magnet link,[16] and Hugging Face.[17] The model was released under the Apache 2.0 license. The release blog post claimed the model outperforms LLaMA 2 13B on all benchmarks tested, and is on par with LLaMA 34B on many benchmarks tested.[18]\\n\\nMistral 7B uses a similar architecture to LLaMA, but with some changes to the attention mechanism. In particular it uses Grouped-query attention (GQA) intended for faster inference and Sliding Window Attention (SWA) intended to handle longer sequences.\\n\\nSliding Window Attention (SWA) reduces the computational cost and memory requirement for longer sequences. In sliding window attention, each token can only attend to a fixed number of tokens from the previous layer in a \"sliding window\" of 4096 tokens, with a total context length of 32768 tokens. At inference time, this reduces the cache availability, leading to higher latency and smaller throughput. To alleviate this issue, Mistral 7B uses a rolling buffer cache.\\n\\nMistral 7B uses grouped-query attention (GQA), which is a variant of the standard attention mechanism. Instead of computing attention over all the hidden states, it computes attention over groups of hidden states.[19]\\n\\nBoth a base model and \"instruct\" model were released with the later receiving additional tuning to follow chat-style prompts. The fine-tuned model is only intended for demonstration purposes, and does not have guardrails or moderation built-in.[18]\\n\\nMixtral 8x7B\\n\\nMuch like Mistral\\'s first model, Mixtral 8x7B was released via BitTorrent on December 9, 2023,[6] and later Hugging Face and a blog post were released two days later.[14]\\n\\nUnlike the previous Mistral model, Mixtral 8x7B uses a sparse mixture of experts architecture. The model has 8 distinct groups of \"experts\", giving the model a total of 46.7B usable parameters.[20][21] Each single token can only use 12.9B parameters, therefore giving the speed and cost that a 12.9B parameter model would incur.[14]\\n\\nMistral AI\\'s testing shows the model beats both LLaMA 70B, and GPT-3.5 in most benchmarks.[22]\\n\\nIn March 2024, research conducted by Patronus AI comparing performance of LLMs on a 100- question test with prompts to generate text from books protected under U.S. copyright law found that Open AI\\'s GPT-4, Mixtral, Meta AI\\'s LLaMA-2, and Anthropic\\'s Claude2 generated\\n\\nhttps://en.wikipedia.org/wiki/Mistral_AI\\n\\n2/5\\n\\n24. 4. 10. 오후 3:22\\n\\nMistral AI - Wikipedia\\n\\ncopyrighted text verbatim in 44%, 22%, 10%, and 8% of responses respectively.[23][24]\\n\\nAPI-Only Models\\n\\nUnlike Mistral 7B and Mixtral 8x7B, the following models are closed-source and only available through the Mistral API.[25]\\n\\nMistral Large\\n\\nMistral Large was launched on February 26, 2024, and Mistral claims it is second in the world only to OpenAI\\'s GPT-4.\\n\\nIt is fluent in English, French, Spanish, German, and Italian, with Mistral claiming understanding of both grammar and cultural context, and provides coding capabilities. As of early 2024, it is Mistral\\'s flagship AI.[26] It is also available on Microsoft Azure.\\n\\nMistral Medium\\n\\nMistral Medium is trained in various languages including English, French, Italian, German, Spanish and code with a score of 8.6 on MT-Bench.[27] It is ranked in performance above Claude and below GPT-4 on the LMSys ELO Arena benchmark.[28]\\n\\nThe number of parameters, and architecture of Mistral Medium is not known as Mistral has not published public information about it.\\n\\nMistral Small\\n\\nLike the Large model, Small was launched on February 26, 2024. It is intended to be a light-weight model for low latency, with better performance than Mixtral 8x7B.[29]\\n\\nReferences\\n\\n1. \"France\\'s unicorn start-up Mistral AI embodies its artificial intelligence hopes\" (https://www.lemond e.fr/en/economy/article/2023/12/12/french-unicorn-start-up-mistral-ai-embodies-its-artificial-intelli gence-hopes_6337125_19.html). Le Monde.fr. 2023-12-12. Retrieved 2023-12-16.\\n\\n2. Metz, Cade (10 December 2023). \"Mistral, French A.I. Start-Up, Is Valued at $2 Billion in Funding Round\" (https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html). The New York Times.\\n\\n3. Fink, Charlie. \"This Week In XR: Epic Triumphs Over Google, Mistral AI Raises $415 Million, $56.5 Million For Essential AI\" (https://www.forbes.com/sites/charliefink/2023/12/14/this-week-in-xr-epic- triumphs-over-google-mistral-ai-raises-415-million-565-million-for-essential-ai/). Forbes. Retrieved 2023-12-16.\\n\\n4. \"A French AI start-up may have commenced an AI revolution, silently\" (https://www.hindustantimes. com/business/a-french-ai-start-up-may-have-commenced-an-ai-revolution-silently-101702370816 617.html). Hindustan Times. December 12, 2023.\\n\\n5. \"French AI start-up Mistral secures €2bn valuation\" (https://www.ft.com/content/ea29ddf8-91cb-45 e8-86a0-f501ab7ad9bb). ft.com Financial Times.\\n\\n6. \"Buzzy Startup Just Dumps AI Model That Beats GPT-3.5 Into a Torrent Link\" (https://gizmodo.com/ mistral-artificial-intelligence-gpt-3-openai-1851091217). Gizmodo. 2023-12-12. Retrieved 2023-12-16.\\n\\n7. \"Bringing open AI models to the frontier\" (https://mistral.ai/news/about-mistral-ai/). Mistral AI. 27 September 2023. Retrieved 4 January 2024.\\n\\nhttps://en.wikipedia.org/wiki/Mistral_AI\\n\\nDL\\n\\n3/5\\n\\n24. 4. 10. 오후 3:22\\n\\nMistral AI - Wikipedia\\n\\n8. \"Open-weight models and Mistral AI Large Language Models\" (https://docs.mistral.ai/models/). docs.mistral.ai. Retrieved 2024-01-04.\\n\\n9. \"Endpoints and Mistral AI Large Language Models\" (https://docs.mistral.ai/platform/endpoints/#medi um). docs.mistral.ai.\\n\\n10. \"Endpoints and benchmarks | Mistral AI Large Language Models\" (https://docs.mistral.ai/platform/en dpoints/). docs.mistral.ai. Retrieved 2024-03-06.\\n\\n11. \"France\\'s unicorn start-up Mistral AI embodies its artificial intelligence hopes\" (https://www.lemond e.fr/en/economy/article/2023/12/12/french-unicorn-start-up-mistral-ai-embodies-its-artificial-intelli gence-hopes_6337125_19.html). Le Monde.fr. 12 December 2023.\\n\\n12. Journal, Sam Schechner | Photographs by Edouard Jacquinet for The Wall Street. \"The 9-Month-Old AI Startup Challenging Silicon Valley\\'s Giants\" (https://www.wsj.com/tech/ai/the-9-month-old-ai-sta rtup-challenging-silicon-valleys-giants-ee2e4c48). WSJ. Retrieved 2024-03-31.\\n\\n13. \"Mistral lève 385 M€ et devient une licorne française - le Monde Informatique\" (https://www.lemon deinformatique.fr/actualites/lire-mistral-leve-385-meteuro-et-devient-une-licorne-francaise-9239 2.html). 11 December 2023.\\n\\n14. \"Mixtral of experts\" (https://mistral.ai/news/mixtral-of-experts/). mistral.ai. 2023-12-11. Retrieved 2024-01-04.\\n\\n15. Bableshwar (2024-02-26). \"Mistral Large, Mistral AI\\'s flagship LLM, debuts on Azure AI Models-as- a-Service\" (https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistral-large-mistral -ai-s-flagship-llm-debuts-on-azure-ai/ba-p/4066996). techcommunity.microsoft.com. Retrieved 2024-02-26.\\n\\n16. Goldman, Sharon (2023-12-08). \"Mistral AI bucks release trend by dropping torrent link to new open source LLM\" (https://venturebeat.com/ai/mistral-ai-bucks-release-trend-by-dropping-torrent- link-to-new-open-source-llm/). VentureBeat. Retrieved 2024-01-04.\\n\\n17. Coldewey, Devin (27 September 2023). \"Mistral AI makes its first large language model free for everyone\" (https://techcrunch.com/2023/09/27/mistral-ai-makes-its-first-large-language-model-fre e-for-everyone/). TechCrunch. Retrieved 4 January 2024.\\n\\n18. \"Mistral 7B\" (https://mistral.ai/news/announcing-mistral-7b/). mistral.ai. Mistral AI. 27 September 2023. Retrieved 4 January 2024.\\n\\n19. Jiang, Albert Q.; Sablayrolles, Alexandre; Mensch, Arthur; Bamford, Chris; Chaplot, Devendra Singh; Casas, Diego de las; Bressand, Florian; Lengyel, Gianna; Lample, Guillaume (2023-10-10). \"Mistral 7B\". arXiv:2310.06825v1 (https://arxiv.org/abs/2310.06825v1) [cs.CL (https://arxiv.org/archive/cs.C L)].\\n\\n20. \"Mixture of Experts Explained\" (https://huggingface.co/blog/moe). huggingface.co. Retrieved 2024-01-04.\\n\\n21. Marie, Benjamin (2023-12-15). \"Mixtral-8x7B: Understanding and Running the Sparse Mixture of Experts\" (https://towardsdatascience.com/mixtral-8x7b-understanding-and-running-the-sparse-mi xture-of-experts-0e3fc7fde818). Medium. Retrieved 2024-01-04.\\n\\n22. Franzen, Carl (2023-12-11). \"Mistral shocks AI community as latest open source model eclipses GPT- 3.5 performance\" (https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source- model-eclipses-gpt-3-5-performance/). VentureBeat. Retrieved 2024-01-04.\\n\\n23. Field, Hayden (March 6, 2024). \"Researchers tested leading AI models for copyright infringement\\n\\nusing popular books, and GPT-4 performed worst\" (https://www.cnbc.com/2024/03/06/gpt-4-resea rchers-tested-leading-ai-models-for-copyright-infringement.html). CNBC. Retrieved March 6, 2024. 24. \"Introducing CopyrightCatcher, the first Copyright Detection API for LLMs\" (https://www.patronus.a\\n\\n24. \"Introducing CopyrightCatcher, the first Copyright Detection API for LLMs” (https://www.patronus.a i/blog/introducing-copyright-catcher). Patronus Al. March 6, 2024. Retrieved March 6, 2024.\\n\\ni/blog/introducing-copyright-catcher). Patronus AI. March 6, 2024. Retrieved March 6, 2024. 25. \"Pricing and rate limits | Mistral AI Large Language Models\" (https://docs.mistral.ai/platform/pricin\\n\\ng/). docs.mistral.ai. Retrieved 2024-01-22.\\n\\n26. AI, Mistral (2024-02-26). \"Au Large\" (https://mistral.ai/news/mistral-large/). mistral.ai. Retrieved 2024-03-06.\\n\\nhttps://en.wikipedia.org/wiki/Mistral_AI\\n\\n4/5\\n\\n24. 4. 10. 오후 3:22\\n\\nMistral AI - Wikipedia\\n\\n27. AI, Mistral (2023-12-11). \"La plateforme\" (https://mistral.ai/news/la-plateforme/). mistral.ai. Retrieved 2024-01-22.\\n\\n28. \"LMSys Chatbot Arena Leaderboard - a Hugging Face Space by lmsys\" (https://huggingface.co/space s/lmsys/chatbot-arena-leaderboard). huggingface.co. Retrieved 2024-01-22.\\n\\n29. AI, Mistral (2024-02-26). \"Au Large\" (https://mistral.ai/news/mistral-large/). mistral.ai. Retrieved 2024-03-06.\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Mistral_AI&oldid=1216522388\"\\n\\nhttps://en.wikipedia.org/wiki/Mistral_AI\\n\\nZ.\\n\\n5/5', metadata={'source': '../dataset/llamaindex_data/Mistral AI - Wikipedia.pdf'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 특정 단위로 문서를 쪼갠 후에 chunk_size이 될 때까지 계속해서 merge하는 방식으로 chunking\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=20, # 인접하는 chunk 사이에 얼만큼 내용이 겹치도록 할 것인지를 정하는 인자.\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1432, which is longer than the specified 500\n",
      "Created a chunk of size 1474, which is longer than the specified 500\n",
      "Created a chunk of size 1440, which is longer than the specified 500\n",
      "Created a chunk of size 998, which is longer than the specified 500\n",
      "Created a chunk of size 1217, which is longer than the specified 500\n",
      "Created a chunk of size 1452, which is longer than the specified 500\n",
      "Created a chunk of size 1422, which is longer than the specified 500\n",
      "Created a chunk of size 1306, which is longer than the specified 500\n",
      "Created a chunk of size 1171, which is longer than the specified 500\n",
      "Created a chunk of size 1425, which is longer than the specified 500\n",
      "Created a chunk of size 881, which is longer than the specified 500\n",
      "Created a chunk of size 1476, which is longer than the specified 500\n",
      "Created a chunk of size 1371, which is longer than the specified 500\n",
      "Created a chunk of size 1387, which is longer than the specified 500\n",
      "Created a chunk of size 1212, which is longer than the specified 500\n",
      "Created a chunk of size 623, which is longer than the specified 500\n",
      "Created a chunk of size 755, which is longer than the specified 500\n",
      "Created a chunk of size 1381, which is longer than the specified 500\n",
      "Created a chunk of size 1182, which is longer than the specified 500\n",
      "Created a chunk of size 1446, which is longer than the specified 500\n",
      "Created a chunk of size 1344, which is longer than the specified 500\n",
      "Created a chunk of size 644, which is longer than the specified 500\n",
      "Created a chunk of size 843, which is longer than the specified 500\n",
      "Created a chunk of size 576, which is longer than the specified 500\n",
      "Created a chunk of size 1393, which is longer than the specified 500\n",
      "Created a chunk of size 747, which is longer than the specified 500\n",
      "Created a chunk of size 1423, which is longer than the specified 500\n",
      "Created a chunk of size 1240, which is longer than the specified 500\n",
      "Created a chunk of size 1383, which is longer than the specified 500\n",
      "Created a chunk of size 679, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# 정해진 chink_size보다 실제 사이즈가 클 경우 해당 내용이 출력된다.\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned.', metadata={'source': '../dataset/llamaindex_data/openai.txt'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter는 separator를 여러개 두고 Recursive하게 split을 진행.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"], # separator를 여러개 둘 경우에 첫번째 separator로 분리하고 길이가 긴 chunk의 경우에는 두번째 separator로 분리하는 식\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=20, # 인접하는 chunk 사이에 얼만큼 내용이 겹치도록 할 것인지를 정하는 인자.\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separator를 여러개 두고 Recursive하게 split을 진행하기 때문에 chunk_size보다 큰 경우가 발생하지 않는다.\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source', metadata={'source': '../dataset/llamaindex_data/openai.txt'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 separator를 사용했기 때문에 문장 중간에 끊긴 경우도 생겼다.\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# SemanticChunker는 문장의 임베딩을 이용하여 chunk를 분할한다. 따라서 임베딩을 필요로 한다.\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "# SemanticChunker는 문장 단위로 임베딩을 생성하며 다음 문장과 현재 문장과의 거리가 특정 threshold 이하일 때 문장들을 결합하여 chunk를 생성하고\n",
    "# 그렇지 않을 경우에는 chunk를 분리한다.\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# CharacterTextSplitter는 token 단위로 chunking이 이루어지며 \n",
    "# LM의 context 제한은 길이 단위가 아니라 token 단위이기 때문에 주로 사용되는 방법이다.\n",
    "\n",
    "# tiktoken encoder는 OpenAI가 개발한 BPE로서 해당 인코더를 사용해서 textsplitter를 정의할 수 있다.\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='OpenAI is a U.S.-based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned.\\n\\nThe new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\\n\\n== History ==', metadata={'source': '../dataset/llamaindex_data/openai.txt'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 단위로 chunking을 했기 때문에 문장이 잘리는 경우가 생긴다.\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embed_model.embed_query(\"What is Mistral AI?\")\n",
    "emb[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embed_model.embed_documents([\"What is Mistral AI?\", \"Hi\", \"ML\"])\n",
    "len(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chunking된 documents와 embedding model을 사용하여 vectorstore를 구성한다.\n",
    "- LangChain에서는 기본적으로 chroma와 FAISS를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vector_index = Chroma.from_documents(documents, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = vector_index.similarity_search(\"What is Mistral AI?\")\n",
    "retrieved[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_index = FAISS.from_documents(documents, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = vector_index.similarity_search(\"What is Mistral AI?\")\n",
    "retrieved[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"OpenAI의 sora7 모델에 대해 알려줘\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmr : Maximum Marginal Relevance\n",
    "# mmr이 search_type의 default 값이다.\n",
    "# 쿼리와 문서들 간의 거리를 최소화하면서 동시에 문서들 사이의 거리를 넓히는 방법\n",
    "# retrieve된 문서 간의 내용의 중복 이슈를 최소화할 수 있다.\n",
    "retriever = vector_index.as_retriever(search_type=\"mmr\")\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_score_threshold\n",
    "# similarity score가 threshold를 넘는 document를 retieval\n",
    "# threshold를 넘는 document가 없는 경우는 어떤 document도 retrieval하지 않는다.\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity_score_threshold\", search_kewargs={\"socre_threshold\": 0.3})\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval하길 원하는 문서의 수를 조절할 수도 있다.\n",
    "retriever = vector_index.as_retriever(search_kwags={\"k\" : 3})\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrivers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 쿼리로부터 여러 개의 쿼리를 만들고 만들어진 여러 쿼리로부터 문서를 retrieval하는 과정은 retirevers의 multi_query를 통해 쉽게 구현할 수 있다.\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_mult = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_index.as_retriever(),\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_mult.get_relevant_documents(query=\"OpenAI의 sora 모델에 대해서 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing 과정에서 chunk_size가 작을수록 retrieval accuracy는 높아지지만 개별 chunk가 전달할 수 있는 맥락은 줄어든다.\n",
    "# chunk_size가 커질수록 개별 chunk가 전달할 수 있는 맥락은 커지지만 retrieval accuracy는 낮아질 수 있다.\n",
    "# ParentDocumentRetriever는 trade-off를 해소하기 위한 방법 중 하나이다.\n",
    "# ParentDocumentRetriever는 두 가지의 다른 chunk_size를 사용하여 chunking을 하고 작은 chunk를 사용하여 retrieval를 진행한 뒤\n",
    "# 실제로 사용할 때는 작은 chunk의 parent를 찾아서 사용하는 방식이다.\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "# 서로 다른 chunk_size를 가진 parent_spliiter와 child_splitter를 정의\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_spliiter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "vector_index = Chroma(collection_name=\"split_parents\", embedding_function=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_index,\n",
    "    docstore=store,\n",
    "    child_splitter=child_spliiter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 단계에서는 문서를 vectorstore에 저장하지 않았기 때문에 어떠한 문서도 retrieval되지 않는다.\n",
    "retriever.get_relevant_documents(query=\"OpenAI의 sora 모델에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever에 docuemnt를 추가한다.\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(query=\"OpenAI의 sora 모델에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"OpenAI의 sora 모델에 대해서 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me something\")\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me something {topic}\")\n",
    "chain = prompt | llm | StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"toplc\" : \"sora, developed by OpenAI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain을 만들어 invoke하면 아래의 과정을 거치게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain에서 chain을 invoke하면 하나의 chain에서의 output을 다음 chain에게 넘겨주는 작업을 반복한다.\n",
    "prompt_formatted = prompt.invoke({\"toplc\" : \"sora, developed by OpenAI\"})\n",
    "prompt_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt를 llm의 invoke 인자로 전달하게 되면 생성된 답변을 출력할 수 있고\n",
    "# 생성된 답변을 OutputParser에 전달하게 되면 정돈된 형태의 답변을 얻을 수 있다.\n",
    "model_output = llm.invoke(prompt_formatted)\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이러한 일련의 과정들을 chain을 구성하면 한번에 할 수 있다는 장점이 있다.\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 Retrieval에 쿼리를 전달하여 관련이 있는 document를 가져오는 작업을 한다.\n",
    "retrieved_docs = retriever.invoke(\"Tell me something about sora, developed by OpenAI\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가져온 문서들을 정돈된 형태로 보기 위한 함수 정의\n",
    "def merge_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in retrieved_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rannable Parallel은 다음 chain에서의 input 형태에 맞게 이전 chain의 output을 관리할 수 있게 한다.\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain_parallel = RunnableParallel({\"context\" : retriever, \"llm\" : llm})\n",
    "chain_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리를 전달하면 해당 쿼리가 retriever에 전달된 결과와 llm에 전달된 결과를 dict 형태로 반환\n",
    "\n",
    "chain_parallel.invoke(\"Tell me something about sora, developed by OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough를 사용하면 input 쿼리를 변경하지 않고 바로 다음 chain에 전달할 수 있다.\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain_parallel = RunnableParallel({\"context\" : retriever, \"query\" : RunnablePassthrough()})\n",
    "chain_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Utilizing the context given below, answer the question.\n",
    "[context]\n",
    "{context}\n",
    "\n",
    "question: {query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\"context\" : retriever, \"query\" : RunnablePassthrough()})\\\n",
    "        | prompt \\\n",
    "        | llm \\\n",
    "        | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Tell me something about sora, develpoed by OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableParallel은 dict 형태로 인자를 전달하는 대신에 keyword argument 형태로도 전달이 가능하다.\n",
    "\n",
    "chain = RunnableParallel(context=retriever, query=RunnablePassthrough()) \\\n",
    "        | prompt \\\n",
    "        | llm \\\n",
    "        | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Tell me something about sora, develpoed by OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(context=retriever, query=RunnablePassthrough()) \\\n",
    "        | prompt\n",
    "\n",
    "chain.invoke(\"Tell me something about sora, develpoed by OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context를 정돈된 형태로 전달하기 위해 merge_docs를 붙여서 사용\n",
    "chain = RunnableParallel({\"context\" : retriever | merge_docs, \"query\" : RunnablePassthrough()})\\\n",
    "        | prompt\n",
    "\n",
    "chain.invoke(\"Tell me something about sora, develpoed by OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hub에서 미리 작성된 프롬프트를 가져와서 사용하는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 임의로 짠 prompt에서는 query였지만, hub에서 가져온 prompt는 question이기 때문에 이를 고려해야 한다.\n",
    "\n",
    "chain = RunnableParallel(context=retriever, question=RunnablePassthrough()) \\\n",
    "        | prompt \\\n",
    "        | llm \\\n",
    "        | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Tell me something about sora, develpoed by OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  5.69it/s]This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.49s/it]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"../dataset/llamaindex_data\", glob=\"*\", show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_index = FAISS.from_documents(documents, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(search_type='mmr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperatue=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_docs(retreived_docs):\n",
    "    return \"\\n\\n\".join([d.page_cotent for d in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\"context\" : retriever | merge_docs, \"quetion\" : RunnablePassthrough()}) \\\n",
    "        | prompt \\\n",
    "        | llm \\\n",
    "        | StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain.invoke(\"Tell me something about sora, develpoed by OpenAI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
